{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxOWPV3dVb-o"
      },
      "source": [
        "# Deep Learning Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SPPOpfQVb-p"
      },
      "source": [
        "## TODO\n",
        "* losses should be weighted for different classes, there are 8 to 9 times of healthy images than there is for bleeding, so model will be inclined to predict healthy\n",
        "* hyperparameters for deep learning runs should be saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1qKSkEWVb-q"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-10T15:50:57.897440108Z",
          "start_time": "2025-01-10T15:50:56.799530900Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pTzdw1FVb-q",
        "outputId": "46fd4df5-e1db-446d-98fd-3732769ef614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
        "from torch.optim import Adam, lr_scheduler\n",
        "from albumentations import (\n",
        "    Compose, HorizontalFlip, VerticalFlip, RandomRotate90,\n",
        "    RandomBrightnessContrast, ShiftScaleRotate, Normalize\n",
        ")\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up5MnmcUVb-r"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-10T15:50:57.901226144Z",
          "start_time": "2025-01-10T15:50:57.897308858Z"
        },
        "id": "rrFasRZaVb-r"
      },
      "outputs": [],
      "source": [
        "class BleedDataset(Dataset):\n",
        "    def __init__(self, root_dir, mode=\"RGB\", augmentations=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.bleeding_dir = os.path.join(root_dir, \"bleeding\")\n",
        "        self.healthy_dir = os.path.join(root_dir, \"healthy\")\n",
        "\n",
        "        # Combine images and labels into tuples\n",
        "        self.data = [\n",
        "            (os.path.join(self.bleeding_dir, p), 1)\n",
        "            for p in os.listdir(self.bleeding_dir)\n",
        "        ] + [\n",
        "            (os.path.join(self.healthy_dir, p), 0)\n",
        "            for p in os.listdir(self.healthy_dir)\n",
        "        ]\n",
        "\n",
        "        self.mode = mode.lower()\n",
        "        if self.mode not in {\"rgb\", \"gray\"}:\n",
        "            raise ValueError(\"Invalid mode. Use 'RGB' or 'gray'.\")\n",
        "\n",
        "        self.augmentations = augmentations  # Store the augmentations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    @staticmethod\n",
        "    def _preprocess_image(image):\n",
        "        image = image[32:544, 32:544]  # Crop black borders\n",
        "        image[:48, :48] = 0  # Remove artifacts\n",
        "        image[:31, 452:] = 0\n",
        "        if image.ndim == 3:\n",
        "            image = np.transpose(image, (2, 0, 1))  # Convert to PyTorch format\n",
        "        else:\n",
        "            image = image[np.newaxis, ...]\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.data[idx]\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE if self.mode == \"gray\" else cv2.IMREAD_COLOR)\n",
        "        image = self._preprocess_image(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI4mXURyVb-r"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-10T15:50:57.918217402Z",
          "start_time": "2025-01-10T15:50:57.900811214Z"
        },
        "id": "-rufaEE1Vb-r"
      },
      "outputs": [],
      "source": [
        "class DummyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DummyModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.conv4 = nn.Conv2d(256, 128, kernel_size=3)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3)\n",
        "        self.drop5 = nn.Dropout2d(p=0.2)\n",
        "        self.conv6 = nn.Conv2d(64, 16, kernel_size=3)\n",
        "        self.drop6 = nn.Dropout2d(p=0.2)\n",
        "        self.relu = F.relu\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.fully_conv = nn.Conv2d(16, 1, kernel_size=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.max_pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.max_pool(self.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = self.max_pool(self.relu(self.bn4(self.conv4(x))))\n",
        "\n",
        "        x = self.drop5(self.relu(self.conv5(x)))\n",
        "        x = self.drop6(self.relu(self.conv6(x)))\n",
        "        x = self.sigmoid(self.fully_conv(x))\n",
        "\n",
        "        return x[:,0,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKbUgmeeVb-r"
      },
      "source": [
        "## Training Space"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA AUGMENTATION\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3eDbU5hGVuQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from albumentations import (\n",
        "    Compose, HorizontalFlip, VerticalFlip, RandomRotate90,\n",
        "    RandomBrightnessContrast, ShiftScaleRotate, Normalize\n",
        ")\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "augmentations = Compose([\n",
        "    HorizontalFlip(p=0.5),\n",
        "    VerticalFlip(p=0.5),\n",
        "    RandomRotate90(p=0.5),\n",
        "    RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "    ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
        "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # Normalization\n",
        "    ToTensorV2()\n",
        "], p=1.0)"
      ],
      "metadata": {
        "id": "TVyhrcEjVskN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvsLGBu0Vb-r"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-10T15:50:57.919580790Z",
          "start_time": "2025-01-10T15:50:57.913228018Z"
        },
        "id": "yzm3zV3AVb-s"
      },
      "outputs": [],
      "source": [
        "SAVE_PATH = \"./\"\n",
        "\n",
        "TRAIN_TEST_SPLIT = (0.8, 0.1) # remaining parts will be test\n",
        "DIRECTORY_PATH = \"../project_capsule_dataset\"\n",
        "BATCH_SIZE = 8\n",
        "LR = 0.001 # learning rate\n",
        "\n",
        "NUM_OF_EPOCHS = 1\n",
        "EARLY_STOP_LIMIT = 3\n",
        "\n",
        "THRESHOLD = 0.5 # predictions bigger than threshold will be counted as bleeding prediction, and lower ones will be healthy prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLk01_QiVb-s"
      },
      "source": [
        "### Dataset, Model etc. Inıtıalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-10T15:50:58.526936237Z",
          "start_time": "2025-01-10T15:50:57.920838409Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "KNXr4taKVb-s",
        "outputId": "148350de-2402-4763-e446-34e3790d6e79"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../project_capsule_dataset/bleeding'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7fbc25b2afe4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     }\n\u001b[0;32m---> 31\u001b[0;31m data_loaders = prepare_datasets(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mBleedDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIRECTORY_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAIN_TEST_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mimage_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-7fbc25b2afe4>\u001b[0m in \u001b[0;36mprepare_datasets\u001b[0;34m(dataset_class, directory, split_ratios, batch_size, image_mode, seed, augmentations)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Dataset Preparation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_ratios\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtotal_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_ratios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7059fdbf7ba3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, mode, augmentations)\u001b[0m\n\u001b[1;32m      8\u001b[0m         self.data = [\n\u001b[1;32m      9\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleeding_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleeding_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhealthy_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../project_capsule_dataset/bleeding'"
          ]
        }
      ],
      "source": [
        "### ---|---|---|---|---|---|---|---|---|---|--- MODEL & DATASET ---|---|---|---|---|---|---|---|---|---|--- ###\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Model Initialization\n",
        "def initialize_model(model_class, save_path):\n",
        "    model = model_class().to(device)\n",
        "    model_serial_number = f\"training_with_{model.__class__.__name__}_{datetime.now().strftime('on_%m.%d._at_%H:%M:%S')}\"\n",
        "    model_serial_path = os.path.join(save_path, model_serial_number)\n",
        "    os.makedirs(model_serial_path, exist_ok=True)\n",
        "    return model, model_serial_path\n",
        "\n",
        "model, model_serial_path = initialize_model(DummyModel, SAVE_PATH)\n",
        "\n",
        "# Dataset Preparation\n",
        "def prepare_datasets(dataset_class, directory, split_ratios, batch_size, image_mode=\"RGB\", seed=0, augmentations=None):\n",
        "    dataset = dataset_class(directory, mode=image_mode, augmentations=augmentations)\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(split_ratios[0] * total_size)\n",
        "    test_size = int(split_ratios[1] * total_size)\n",
        "    validation_size = total_size - train_size - test_size\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    train_dataset, validation_dataset, test_dataset = random_split(dataset, [train_size, validation_size, test_size])\n",
        "\n",
        "    return {\n",
        "        \"train\": DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True),\n",
        "        \"validation\": DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, pin_memory=True),\n",
        "        \"test\": DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True),\n",
        "    }\n",
        "data_loaders = prepare_datasets(\n",
        "    BleedDataset, DIRECTORY_PATH, TRAIN_TEST_SPLIT, BATCH_SIZE,\n",
        "    image_mode=\"RGB\", seed=0, augmentations=augmentations\n",
        ")\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pny9nWPVb-s"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-10T15:52:40.628953950Z",
          "start_time": "2025-01-10T15:50:58.526428978Z"
        },
        "id": "Ri9bChvdVb-s",
        "outputId": "cfc6ef2b-2e94-4bbb-f341-9afc90c38893"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                        \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     averaged_training_loss \u001b[38;5;241m=\u001b[39m averaged_training_loss \u001b[38;5;241m+\u001b[39m train_loss\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m     \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# calculating the average loss in this epoch's training loop\u001b[39;00m\n",
            "File \u001b[0;32m~/Downloads/project/venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/project/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Downloads/project/venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "### ---|---|---|---|---|---|---|---|---|---|--- TRAINING ---|---|---|---|---|---|---|---|---|---|--- ###\n",
        "train_losses, validation_losses = [], []\n",
        "min_validation_loss = None # minimum achieved loss on validation dataset, used for early stopping\n",
        "min_validation_path = None # path to model checkpoint file\n",
        "early_stop_step = 0\n",
        "\n",
        "for epoch in range(NUM_OF_EPOCHS):\n",
        "    averaged_training_loss = 0\n",
        "    for batch_idx, (images, labels) in tqdm(enumerate(data_loaders['train']), leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        model = model.train()\n",
        "        outputs = model(images.type(torch.float))\n",
        "\n",
        "        float_outputs = outputs[:,0].type(torch.float)\n",
        "        float_labels = labels.type(torch.float)\n",
        "        train_loss = criterion(float_outputs, float_labels)\n",
        "        averaged_training_loss = averaged_training_loss + train_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # calculating the average loss in this epoch's training loop\n",
        "    averaged_training_loss = averaged_training_loss / len(data_loaders['train'])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model = model.eval()\n",
        "\n",
        "        # calculate validation loss\n",
        "        validation_loss = 0.0\n",
        "        for validation_images, validation_labels in data_loaders['validation']:\n",
        "            validation_images, validation_labels = validation_images.to(device), validation_labels.to(device)\n",
        "\n",
        "            validation_outputs = model(validation_images.type(torch.float))\n",
        "\n",
        "            float_validation_outputs = validation_outputs[:,0].type(torch.float)\n",
        "            float_validation_labels = validation_labels.type(torch.float)\n",
        "            validation_loss += criterion(float_validation_outputs, float_validation_labels).item()\n",
        "\n",
        "        # and average the loss over dataset length\n",
        "        validation_loss /= len(data_loaders['validation'])\n",
        "\n",
        "    # if this is first validation or a new minimum is achieved\n",
        "    if min_validation_loss is None or validation_loss < min_validation_loss:\n",
        "        early_stop_step = 0\n",
        "        min_validation_loss = validation_loss\n",
        "\n",
        "        # if there is a checkpoint file, remove it\n",
        "        if min_validation_path is not None:\n",
        "            os.remove(min_validation_path)\n",
        "\n",
        "        # save the new checkpoint file\n",
        "        min_validation_path = os.path.join(model_serial_path, \"min_validation_loss:\"+str(min_validation_loss) + \"_epoch:\" + str(epoch) + \".pth\")\n",
        "        torch.save(model.state_dict(), min_validation_path)\n",
        "\n",
        "    # log the losses, and append to the lists\n",
        "    print(f\"Epoch: {epoch+1} | training loss: {averaged_training_loss.item()} | min validation loss: {min_validation_loss}\", flush=True)\n",
        "    train_losses.append(averaged_training_loss.item())\n",
        "    validation_losses.append(validation_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    # check for early stopping\n",
        "    early_stop_step = early_stop_step + 1\n",
        "    if early_stop_step == EARLY_STOP_LIMIT:\n",
        "        print(\"early stopping...\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-01-10T15:52:40.628308081Z"
        },
        "id": "4kE65GWtVb-s"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, color='blue', label='Train Loss')\n",
        "plt.plot(validation_losses, color='orange', label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(model_serial_path, \"losses.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcCSO7Z_Vb-s"
      },
      "source": [
        "## Testing Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-10T15:52:40.630076269Z",
          "start_time": "2025-01-10T15:52:40.629125510Z"
        },
        "id": "9ZVrBJsKVb-s"
      },
      "outputs": [],
      "source": [
        "### ---|---|---|---|---|---|---|---|---|---|--- TESTING ---|---|---|---|---|---|---|---|---|---|--- ###\n",
        "loaded_model = DummyModel().to(device)\n",
        "#loaded_model.load_state_dict(torch.load(min_validation_path))\n",
        "loaded_model = loaded_model.eval()\n",
        "\n",
        "# class_correct counts how many correct predictions for that label [corrects_for_label_0, corrects_for_label_1]\n",
        "# class_total counts how many predictions are there for that label [predictions_for_label_0, predictions_for_label_1]\n",
        "class_correct, class_total = [0,0], [0,0]\n",
        "with torch.no_grad():\n",
        "    for test_images, test_labels in tqdm(data_loaders['test']):\n",
        "        test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
        "\n",
        "        test_outputs = loaded_model(test_images.type(torch.float))\n",
        "\n",
        "        test_outputs = test_outputs[:,0].type(torch.float)\n",
        "        test_outputs[test_outputs >= THRESHOLD] = 1\n",
        "        test_outputs[test_outputs < THRESHOLD] = 0\n",
        "\n",
        "        # calculate indices for correct predictions\n",
        "        correct = (test_outputs == test_labels).squeeze()\n",
        "        for e, label in enumerate(test_labels):\n",
        "            # increase the correct prediction count for that label\n",
        "            class_correct[label] += correct[e].item()\n",
        "            class_total[label] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-01-10T15:52:40.629693529Z"
        },
        "id": "709owi4wVb-t"
      },
      "outputs": [],
      "source": [
        "# Total: accuracy for whole dataset\n",
        "print(f\"Total accuracy: {sum(class_correct)/sum(class_total)} on threshold: {THRESHOLD}\")\n",
        "print(f\"Healthy detection: {class_correct[0]}/{class_total[0]} | accuracy: {class_correct[0]/class_total[0]}\")\n",
        "print(f\"Bleeding detection: {class_correct[1]}/{class_total[1]} | accuracy: {class_correct[1]/class_total[1]}\")\n",
        "\n",
        "with open(os.path.join(model_serial_path, \"accuracy.txt\"), 'w') as txt:\n",
        "    txt.write(f\"Total accuracy: {sum(class_correct)/sum(class_total)} on threshold: {THRESHOLD}\\n\")\n",
        "    txt.write(f\"Healthy detection: {class_correct[0]}/{class_total[0]} | accuracy: {class_correct[0]/class_total[0]}\\n\")\n",
        "    txt.write(f\"Bleeding detection: {class_correct[1]}/{class_total[1]} | accuracy: {class_correct[1]/class_total[1]}\\n\")\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "490-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}